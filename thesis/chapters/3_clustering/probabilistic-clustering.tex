Los algoritmos de clustering probabilísticos modelan el conjunto de datos a partir de la asunción de que estos son generados a partir de la combinación de determinadas distribuciones de probabilidad.
Estos algoritmos transforman el problema de clustering en el de estimar los parámetros para $K$ distribuciones de probabilidad.
Luego los puntos del conjunto de datos que se correspondan con una misma distribución se asociarán al mismo cluster.

En esta sección analizamos un modelo de clustering probabilístico ampliamente estudiado, conocido como \textit{Gaussian Mixture Model} (GMM) y la técnica \textit{Expectation-maximization}, empleada para estimarlo computacionalmente.

\subsection{Combinación de modelos}\label{subsec:mixtureModels}

Sea $X={x_1,\dots,x_N}$ un conjunto de datos de $N$ observaciones de una variable aleatoria $x$ con $D$ dimensiones.
Asumimos que la variable $x_i$ sigue una distribución consistente con la combinación de $K$ \textit{distribuciones componentes} (clusters), cada una instancia de una distribución para determinados parámetros.
Podemos definir entonces la función de densidad de $x_i$ como:

\begin{equation}
    \label{eq:mixtureModels}
    p(x_i)=\sum_{k=1}^{K}{\pi_k p(x_i|\theta_k)}
\end{equation}

\noindent
donde cada $\theta_k$ es el conjunto de parámetros específicos de la $k$-ésima componente y $p(x_i|\theta_k)$ su función de densidad.
Los pesos $\pi_k$, también conocidos como \textit{mixing probabilities}, deben satisfacer las condiciones $0\leq \pi_k \leq 1$ y $\sum_{k=1}^{K}{\pi_k}=1$.

Si bien la definición no establece ninguna restricción en cuanto al tipo de distribución que debe seguir cada componente;
en la práctica, para simplificar el estudio de estos modelos, suele asociarse una misma distribución a todas las componentes, variando únicamente sus parámetros.

\subsection{Gaussian Mixture Model}\label{subsec:GMM}

El modelo de clustering probabilístico más extendido es el de combinación de distribuciones normales, conocido en la literatura como \textit{Gaussian Mixture Model} (GMM)~\cite{Murphy12}.
Es asimismo, uno de los modelos de mayor uso en aplicaciones relacionadas con el análisis acústico. %TODO cite

La distribución normal, en el caso de una variable unidimensional $x$, tiene una función de densidad de la forma:

\begin{equation}
    \label{eq:singleGaussian}
    \mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp{(-\frac{1}{2\sigma^2}((x-\mu)^2)}
\end{equation}

\noindent
donde $\mu$ es la media y $\sigma^2$ la varianza.
Para el caso de $D$ dimensiones, la función toma la forma:

\begin{equation}
    \label{eq:multidimGaussian}
    \mathcal{N}(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp{(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))}
\end{equation}

\noindent
donde $\mu$ es el vector $D$-dimensional de medias y $\Sigma$, de dimensión $D\times D$, la matriz de covarianza con determinante $|\Sigma|$.

En GMM cada componente corresponde a una distribución normal con determinados valores asociados a sus parámetros $\mu$ y $\Sigma$.
A partir de la ecuación~(\ref{eq:mixtureModels}) podemos entonces formular este modelo como:

\begin{equation}
    \label{eq:GMM}
    p(x_i|\Theta) = p(x_i|\pi,\mu,\Sigma)= \sum_{k=1}^{K}{\pi_k \mathcal{N}(x_i|\mu_k,\Sigma_k)}
\end{equation}

Para estimar los parámetros de un modelo, podemos emplear el método de máxima verosimilitud.
Dado un conjunto de observaciones $X$, la función de log-verosimilitud se define como:

\begin{equation}
    \label{eq:log-likelihood}
    l(\Theta|X) = \log{p(X|\Theta)} = \sum_{i=1}^{N}{\log{p(x_i|\Theta)}} = \sum_{i=1}^{N}{\log{\sum_{k=1}^{K}{\pi_k \mathcal{N}(x_i|\mu_k,\Sigma_k)}}}
\end{equation}

El método de máxima verosimilitud estima $\Theta$ como el valor que maximiza~(\ref{eq:log-likelihood}).
Para encontrar dicho valor, podemos computar las derivadas parciales de~(\ref{eq:log-likelihood}) respecto a $\pi_k$, $\mu_k$, y $\Sigma_k$ respectivamente.
Si igualamos a cero la derivada respecto a $\mu_k$ podemos despejar la siguiente expresión:

\begin{equation}
    \label{eq:mu_k}
    \mu_k = \frac{\sum_{i=1}^{N}{\gamma(z_{ik})x_i}}{\sum_{i=1}^{N}{\gamma(z_{ik})}}
\end{equation}

\noindent
donde

\begin{equation}
    \label{eq:gamma}
    \gamma(z_{ik}) = \frac{\pi_k \mathcal{N}(x_i|\mu_k,\Sigma_k)}{\sum_{j=1}^{K}{\pi_j \mathcal{N}(x_i|\mu_j,\Sigma_j)}}
\end{equation}

$\gamma(z_{ik})$ es conocida como la \textbf{responsabilidad} de la componente $k$ sobre la observación $i$-ésima $x_i$.

De igual forma, igualando a cero la derivada de~(\ref{eq:log-likelihood}) respecto a $\Sigma_k$, obtendremos

\begin{equation}
    \label{eq:Sigma_k}
    \Sigma_k = \frac{\sum_{i=1}^{N}{\gamma(z_{ik})(x_i-\mu_k)(x_i-\mu_k)^T}}{\sum_{i=1}^{N}{\gamma(z_{ik})}}
\end{equation}

Luego de aplicar algunas operaciones adicionales~\cite{Aggarawal13} debido a las características de las restricciones a las que están sujetos los pesos $\pi_k$, se llega a

\begin{equation}
    \label{eq:pi_k}
    \pi_k = \frac{\sum_{i=1}^{N}{\gamma(z_{ik})}}{N}
\end{equation}

Sin embargo, la optimización de la función~(\ref{eq:log-likelihood}) presenta serios inconvenientes debido a la dependencia existente entre las responsabilidades $\gamma(x_{ik})$ y el resto de los parámetros, por lo que no resulta simple derivar una expresión que permita calcular directamente dichos valores.
Generalmente solo pueden ser obtenidos mínimos locales aplicando algoritmos de descenso por gradiente sobre la expresión~(\ref{eq:log-likelihood}), lo que igualmente resulta difícil debido a las restricciones del modelo (matriz de covarianzas definida positiva, suma de los pesos $\pi_k$ igual a uno, etc.)~\cite{Aggarawal13,Murphy12}.

\subsection{Expectation-maximization}\label{subsec:EM}

El algoritmo \textit{Expectation-maximization} (EM) permite estimar parámetros de máxima verosimilitud para un modelo.
Sigue un proceso iterativo que alterna dos etapas: se infieren valores para los parámetros (\textit{expectation}), y luego se optimizan dichos valores para el conjunto de datos dado (\textit{maximization}).

En el caso particular de GMM, podemos aplicarlo empleando para ello las ecuaciones que obtenidas en la sección~\ref{subsec:GMM} como se explica en el algoritmo~\ref{algorithm:EM}.

\begin{algorithm}
    \caption{Expectation-maximization para GMM}
    \label{algorithm:EM}
    Inicializar $\mu_k^0$, $\Sigma_k^0$, y $\pi_k^0$\;
    \Repeat{Se cumple criterio de convergencia}{
    \textbf{Expectation}: Calcular las responsabilidades $\gamma(z_{ik})$ sustituyendo los valores actuales de los parámetros en~(\ref{eq:gamma})\;
    \textbf{Maximization}: Actualizar los parámetros sustituyendo las responsabilidades actuales en las expresiones (\ref{eq:mu_k}), (\ref{eq:Sigma_k}) y (\ref{eq:pi_k}).
    Las nuevas medias son usadas al calcular las covarianzas\;
    }
\end{algorithm}

Usualmente se toma como criterio de convergencia para este algoritmo que la variación de la log-verosimilitud respecto a la iteración anterior haya sido menor que un valor $\epsilon$ determinado, o se haya superado una cantidad máxima de iteraciones.

La cantidad de iteraciones requeridas por EM para converger es mayor que las que toma K-Means~\cite{Park09}.
Para acelerar la ejecución de este algoritmo, generalmente se realiza una corrida de K-Means sobre el conjunto de datos, y se toman las medias, varianzas y proporción de puntos en los clusters para inicializar los parámetros $\mu_k^0$, $\Sigma_k^0$ y $\pi_k^0$ respectivamente.

De forma similar a lo que ocurre con el algoritmo K-Means, EM puede caer en máximos locales en dependencia de los valores iniciales.

% TODO Add explaination on how to determine number of components (chosing the right model)

\subsubsection{Complejidad espacial y temporal}

El análisis de los requerimientos de memoria de Expectation-maximization no difiere sustancialmente del de K-Means.
Solamente los puntos del conjunto de datos y las variables deben ser almacenados para la ejecución del algoritmo.
No obstante, a diferencia de K-Means donde la única variable asociada al algoritmo era la posición de los centroides, en EM el espacio de memoria ocupado es mayor, principalmente a causa de la matriz de covarianza.
Las dimensiones de esta última varían en dependencia del modo en que sea analizada la covarianza, ya sea individual para cada componente o global, pudiendo ir desde $O(K)$ hasta $O(K \cdot m^2)$.
En el caso peor, la cantidad de memoria es por tanto $O(nm + K \cdot m^2)$.

El tiempo requerido por el algoritmo es igualmente $O(I\cdot K\cdot m\cdot n)$, siendo $I$ el número de iteraciones necesarias para la convergencia. %TODO cite!
La principal diferencia en este caso radica en el hecho de la cantidad de iteraciones ejecutadas por el algoritmo, que es muy~\footnote{¡Superlativamente!} distinta a la de K-Means, y tiende a ser un valor considerablemente elevado~\cite{Park09}.

% TODO Consider Variational Bayesian Gaussian Mixture