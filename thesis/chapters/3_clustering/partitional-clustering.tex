A continuación es discutido el algoritmo de clustering conocido como K-Means, uno de los más simples y eficientes existentes en la literatura.
Luego de describir en detalle el algoritmo, se analizan algunos de los principales factores que influyen sobre sus resultados.
Finalmente, se presenta una variación de K-Means que busca disminuir la complejidad computacional del algoritmo.

\subsection{K-Means}\label{subsec:k-means}

K-Means~\cite{MacQueen67} es el algoritmo de clustering particional más empleado~\cite{Aggarawal13}.
Comienza seleccionando $K$ puntos representativos como \textit{centroides} iniciales, donde $K$ es un parámetro manualmente especificado por el usuario, siendo este el número deseado de clusters a obtener.
Cada punto del conjunto de datos es luego asignado al centroide más cercano basándose en una medida de proximidad determinada.
Una vez se han formado los clusters, los centroides para cada cluster son actualizados a un nuevo punto.
De manera iterativa, el algoritmo repite estos dos pasos hasta que los centroides no cambien o algún criterio de convergencia alternativo sea cumplido.
K-Means es un algoritmo \textit{greedy} con convergencia garantizada a un mínimo local~\cite{Selim84} pero, visto como un problema de optimización, ha sido demostrado que hallar el mínimo de su función objetivo es NP-Hard~\cite{Manning08}.
En la práctica, suele usarse como criterio de convergencia una versión relajada, continuándose las iteraciones hasta que menos del 1\% de los puntos cambien de cluster.

\begin{algorithm}
    \caption{K-Means}
    \label{algorithm:KMeans}
    Seleccionar $K$ puntos como centroides\;
    \Repeat{Se cumple criterio de convergencia}{
    Formar $K$ clusters asignando cada punto al centroide más próximo\;
    Recomputar el centroide de cada cluster\;
    }
\end{algorithm}

La elección de la medida de proximidad para calcular el centroide más próximo a cada punto puede afectar significativamente las asignaciones y la calidad de la solución final.
Medidas como la distancia Manhattan (norma $L_1$), la distancia euclidiana (norma $L_2$) y la similitud coseno son frecuentemente empleadas, especialmente la segunda.
Tanto la medida de proximidad como el valor de $K$ son determinantes en la configuración de clusters producida por K-Means.

Si se analiza este algoritmo como un problema de optimización, entonces estaría minimizándose la función objetivo de K-Means conocida como Suma de Errores Cuadráticos (SSE por sus siglas en inglés), cuya formulación matemática se presenta a continuación.

Dado un conjunto de datos $D={x_1,x_2,\dots,x_N}$ de $N$ puntos, y denotado el conjunto de clusters obtenido tras aplicar K-Means como $C={C_1,C_2,\dots,C_k,\dots,C_K}$;
la SSE para $C$ es definida en la ecuación~(\ref{eq:SSE}) donde $c_k$ es el centroide del cluster $C_k$.

\begin{equation}
    \label{eq:SSE}
    SSE(C)=\sum_{k=1}^{K}{\sum_{x_{i}\in C_k}{dist(x_i, c_k)^2}}
\end{equation}

En otras palabras, se calcula el error de cada punto de los datos, es decir, su distancia al centroide más próximo, y luego es computada la suma de los cuadrados de dichos errores.
Dados dos conjuntos de clusters obtenidos aplicando dos diferentes corridas de K-Means, sería preferible conservar el de menor SSE puesto que esto significaría que los centroides hallados en esa corrida constituyen una mejor representación de los puntos en sus clusters.
De ahí que el resultado de minimizar la función SSE represente el conjunto de clusters óptimo.

Los centroides que minimizan la SSE son la media de los puntos de cada cluster~\cite{Tan05}.
El centroide del $k$-ésimo cluster, $c_k$, quedaría entonces definido según la ecuación~(\ref{eq:centroid}).

\begin{equation}
    \label{eq:centroid}
    c_{k}=\frac{1}{|C_k|}\sum_{x_{i}\in C_k}{x_i}
\end{equation}

Los pasos 3 y 4 del algoritmo~\ref{algorithm:KMeans} directamente intentan minimizar la SSE. El paso 3 forma clusters asignando los puntos al centroide más cercano, lo que minimiza el SSE de dicho conjunto de centroides.
Asimismo, el paso 4 recomputa los centroides, produciendo un nuevo conjunto de menor SSE, en correspondencia con la ecuación ~(\ref{eq:centroid}).
Sin embargo, como se mencionó anteriormente, estos pasos solamente garantizan la convergencia de K-Means a un mínimo local de la función SSE, puesto que la optimizan partiendo de una selección específica de centroides y cantidad de estos, en lugar de todas las posibles opciones.

\subsubsection{Selección de centroides iniciales}

En~\cite{MacQueen67} se propone un simple método de inicialización consistente en seleccionar los $K$ centroides de modo aleatorio.
Este es ampliamente usado en la literatura por su sencillez, aunque tiene la desventaja de que puede producir resultados muy diferentes en varias corridas del algoritmo, algunos de mayor calidad que otros.

Se han popularizado otras variantes de selección de los centroides, con el propósito de aumentar la efectividad y consistencia en los resultados de K-Means.
Uno de ellos es tomar una muestra de puntos y agruparlos empleando una técnica de clustering jerárquico.
Una vez formados $K$ clusters, se toman sus centroides y se inicializa K-Means con estos.
Este enfoque a menudo ofrece buenos resultados, pero solamente resulta práctico si la muestra tomada es relativamente pequeña (de un orden entre $10^2$ y $10^3$) y $K$ es relativamente pequeño comparado con el tamaño de dicha muestra~\cite{Tan05}.

Otra variante, conocida como \textbf{K-Means++}~\cite{Arthur07}, consiste en primeramente seleccionar un punto de manera aleatoria o tomando el centroide de todos los puntos.
Luego, se selecciona el punto más alejado de los centroides formados con anterioridad y se repite este paso hasta obtener $K$ centroides iniciales.

\subsubsection{Estimación del número de clusters}

K-Means es un algoritmo extremadamente dependiente del valor de $K$ seleccionado por el usuario.
La decisión de tal número constituye uno de los mayores desafíos, si no el mayor, al hacer uso de este algoritmo.
Es por esto que numerosos trabajos se han enfocado en el área de determinar el $K$ más apropiado, y varios métodos han sido desarrollados con tal propósito.
A continuación se mencionan algunos de los más generalizados.

\begin{enumerate}
    \item \textbf{Índice de Calinski-Harabasz}~\cite{Calinski74}: Está definido por la ecuación~(\ref{eq:CH}):
    \begin{equation}
        \label{eq:CH}
        CH(K)=\frac{\frac{B(K)}{K-1}}{\frac{W(K)}{N-K}}
    \end{equation}
    donde $N$ representa la cardinalidad del conjunto de datos.
    El número de clusters es seleccionado maximizando la función dada en la ecuación~(\ref{eq:CH}).
    $B(K)$ y $W(K)$ constituyen las sumas de los cuadrados de las distancias intra e inter-cluster respectivamente (dados $K$ clusters).

    \item \textbf{Estadística de Brecha}\footnote{\textit{Gap Statistic} en inglés.}~\cite{Tibshirani01}: En este método se generan $B$ conjuntos de datos que sigan una distribución uniforme en el mismo intervalo que el original.
    Sea $W_{b}^{*}(K)$ la suma de los cuadrados de las distancias intra-cluster del $b$-ésimo conjunto de datos, se plantea entonces la siguiente ecuación:
    \begin{equation}
        Gap(K) = \frac{1}{B} \times\sum_{b}{\log(W_{b}^{*}(K)) - \log(W(K))}
    \end{equation}
    El número de clusters seleccionado es el menor valor de $K$ que satisfaga la ecuación~(\ref{eq:Gap}):
    \begin{equation}
        \label{eq:Gap}
        Gap(K) \geq Gap(K+1) - S_{k+1}
    \end{equation}
    donde $S_{k+1}$ es el valor de la desviación estándar de $\log(W_{b}^{*}(K+1))$.

    \item \textbf{Criterio de Información de Akaike (AIC)}~\cite{Yeung01}: Sea $M$ el número de dimensiones del conjunto de datos, $K$ se calcula a partir de la ecuación~(\ref{eq:AIC}).
    \begin{equation}
        \label{eq:AIC}
        K=argmin_{K}[SSE(K)+2M K]
    \end{equation}

    \item \textbf{Coeficiente de Silueta}\footnote{\textit{Silhouette Coefficient} en inglés.}~\cite{Kaufman90}: Su formulación considera tanto la distancia intra-cluster como la inter-cluster.
    Para un punto dado $x_i$, primero se calcula el promedio de las distancias de este a todos los puntos del mismo cluster ($a_i$).
    Luego por cada cluster que no contiene a $x_i$, se computa el promedio de las distancias de $x_i$ a sus integrantes ($b_i$).
    Usando estos dos valores, se estima el coeficiente de silueta de un punto como el cociente entre su diferencia y el mayor de ambos.
    El promedio de todos los coeficientes en el conjunto de datos puede ser empleado para evaluar la calidad de un clustering.
    Mayores valores se corresponden con modelos cuyos clusters se encuentran mejor definidos.
    \begin{equation}
        S = \frac{\sum_{i=1}^{N}{\frac{b_{i}-a_{i}}{\max(a_i,b_i)}}}{N}
    \end{equation}
\end{enumerate}

\subsubsection{Complejidad espacial y temporal}

Los requerimientos de espacio de memoria para K-Means son relativamente pequeños puesto que solamente los puntos de datos y los centroides son almacenados por el algoritmo.
Específicamente, la cantidad de memoria empleada es $O((n+K)m)$, donde $n$ es el número de puntos y $m$ la cantidad de atributos (dimensionalidad) de estos.
Los requisitos de tiempo de este algoritmo son igualmente bajos, es básicamente lineal respecto al tamaño del conjunto de datos.
En particular, el tiempo requerido es $O(I \cdot K \cdot m \cdot n)$, donde $I$ es el número de iteraciones necesarias para converger.
A menudo $I$ es suficientemente pequeño, y usualmente puede ser considerado como un valor constante y despreciable.
De esta forma, K-Means es lineal respecto al tamaño del conjunto de datos $n$, y es muy eficiente siempre que el número de clusters $K$ sea significativamente menor que $n$~\cite{Tan05}.

\subsection{Mini-batch K-Means}\label{subsec:miniBatchKMeans}

Mini-batch K-Means es una variante del algoritmo K-Means que emplea \textit{mini-batches} con el fin de reducir el tiempo de computación, sin afectar la función objetivo a optimizar.
Los \textit{mini-batches} son subconjuntos del conjunto de datos sobre el que se aplica el algoritmo, tomados mediante un muestreo aleatorio en cada iteración.
Estos reducen drásticamente la cantidad de cómputo necesario para converger a un óptimo local.

\begin{algorithm}
    \caption{Mini-batch K-Means}
    \label{algorithm:MiniBatchKMeans}
    Seleccionar $K$ puntos como centroides\;
    \Repeat{Se cumple criterio de convergencia}{
    Tomar una muestra aleatoria de $b$ puntos del conjunto de datos\;
    Asignar cada punto de la muestra al cluster que corresponda al centroide más próximo\;
    Recomputar el centroide de cada cluster\;
    }
\end{algorithm}

Al recomputar los centroides durante cada iteración se tienen en cuenta tanto los puntos de la muestra recién asignados como los asignados durante las iteraciones anteriores.

El algoritmo Mini-batch K-Means converge a mayor velocidad que K-Means, aunque la calidad de sus resultados es menor.
No obstante, para aplicaciones prácticas, esta diferencia en calidad suele ser poco significativa~\cite{Sculley10}.