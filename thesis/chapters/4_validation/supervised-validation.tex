Las medidas de validación supervisadas o \textbf{externas}, parten de la premisa de que se disponen las categorías reales de los elementos del conjunto de datos sobre el que se aplicó el algoritmo de clustering.
Haciendo uso de este conocimiento, pueden construirse clusters ideales, estableciendo una correspondencia biyectiva entre clusters y categorías.
La tarea de los criterios de validación supervisados es comparar el resultado de la aplicación de un algoritmo de clustering con dicho conjunto de clusters ideales.

De acuerdo al enfoque que adopta cada criterio para realizar la comparación, estos pueden agruparse en dos conjuntos:

\begin{itemize}
    \item \textbf{Orientados a la clasificación}: Conjunto de aquellas medidas que evalúan la composición de los clusters respecto a qué tan cerca se encuentran de contener elementos pertenecientes a una única clase.
    \item \textbf{Orientados a la similaridad}: Estos criterios valoran cuánto se cumple que dos objetos de una misma categoría se encuentren en el mismo cluster y viceversa.
\end{itemize}

\subsection{Medidas orientadas a la clasificación}\label{subsec:medidasOrientadasALaClasificación}

Este grupo de medidas constituye una adaptación de criterios empleados tradicionalmente para la evaluación de algoritmos de aprendizaje supervisado.
A continuación se mencionan algunas de ellas.

\subsubsection{Entropía}

La \textit{entropía}~\cite{Tan05,Wagner07} de un conjunto es el grado en que está constituido por elementos de una única clase.
La entropía de un conjunto de datos $X$ cuyos elementos se encuentran categorizados por el conjunto de clases $L$ puede calcularse mediante la fórmula:

\begin{equation}
    \label{eq:entropy}
    H(X, L) = -\sum_{l=1}^{|L|}{\frac{|X_l|}{|X|}\log{\left( \frac{|X_l|}{|X|} \right)}}
\end{equation}

\noindent
donde $|L|$ es el total de categorías en que están agrupados los elementos del conjunto, $|X_l|$ la cantidad de elementos que pertenecen a la categoría $l$, y $|X|$ el total de elementos del conjunto.

De igual manera, la entropía de un conjunto de clusters conocidas las categorías reales a las que pertenecen sus elementos, se puede calcular aplicando la siguiente expresión:

\begin{equation}
    \label{eq:clustering-entropy-conditional}
    H(X, K|C) = -\sum_{i=1}^{|K|}\sum_{j=1}^{|C|}{\frac{n_{ij}}{n}\log \left( \frac{n_{ij}}{n_i} \right)}
\end{equation}

\noindent
donde $K$ es el conjunto de clusters, $C$ el conjunto de categorías, $n$ total de elementos del conjunto de datos $X$, $n_i$ la cantidad de elementos del cluster $i$, y $n_{ij}$ el número de elementos que pertenecen simultáneamente al cluster $i$ y la categoría $j$.
Esta expresión puede simplificarse si es vista como la suma de las entropías de cada uno de los clusters, ponderada por los tamaños de estos, convirtiéndose así en:

\begin{equation}
    \label{eq:clustering-entropy}
    e = H(X, K|C) = \sum_{i=1}^{|K|}{\frac{n_i}{n}H(K_i,C)}
\end{equation}

\noindent
donde $K_i$ es el subconjunto del conjunto de datos constituido por los elementos que forman parte del cluster $i$.

Los valores de este medidor se encuentran en el rango $[0, \log|C|]$, siendo 0 el indicador de que todos los elementos se agruparon correctamente, mientras que mayores valores indican peor agrupamiento.

\subsubsection{Pureza, Precisión y Recobrado}

La \textbf{pureza}~\cite{Aggarawal13,Tan05} de un cluster mide lo cerca que este se encuentra de contener objetos de una única categoría.
Se define por las fórmulas:

\begin{gather}
    \label{eq:cluster-purity}
    p_i =\max_{j}{p_{ij}} \\
    \label{eq:purity}
    purity = \sum_{i=1}^{K}{\frac{n_i}{n}p_i}
\end{gather}

\noindent
donde $K$ es la cantidad de clusters, y $p_{ij} = n_{ij}/n_i$ es la probabilidad de que un elemento del cluster $i$ pertenezca a la categoría $j$.
La expresión (\ref{eq:cluster-purity}) determina la pureza de un cluster, y~(\ref{eq:purity}) la de un conjunto de clusters.

La \textbf{precisión}~\cite{Aggarawal13,Tan05} es la fracción de los elementos de un cluster que pertenecen a la misma clase.
La precisión del cluster $i$ respecto a la clase $j$ está dada por la expresión $precision(i,j) = p_{ij}$.

El \textbf{recobrado}~\cite{Aggarawal13,Tan05} mide el grado en que un cluster contiene todos los elementos que pertenecen a una clase dada.
Para el cluster $i$ y la categoría $j$, su expresión es $recall(i,j) = n_{ij}/n_j$, donde $n_j$ es el número de elementos del conjunto de datos incluidos en la clase $j$.

Los tres criterios mencionados toman valores entre 0 y 1, donde un valor más alto indica un mejor resultado.

\subsubsection{Medida F}

Haciendo uso de los conceptos de precisión y recobrado, se define la medida F~\cite{Aggarawal13,Tan05,Wagner07} como una combinación de estos, que busca medir el grado en que un cluster está constituido solamente por elementos de una única clase y a su vez contiene a todos los elementos de dicha clase.
La medida F del cluster $i$ respecto a la clase $j$ está dada por:

\begin{equation}
    \label{eq:F-measure}
    F(i,j) = \frac{2 \cdot precision(i,j) \cdot recall(i,j)}{precision(i,j) + recall(i,j)}
\end{equation}

Los valores de la medida F oscilan en el intervalo $[0,1]$, correspondiéndose los valores más altos con los mejores resultados.

\subsubsection{Homogeneidad, Completitud y Medida V}

Con propósitos semejantes a los trazados por la precisión y el recobrado, pero generalizando la información para todas las categorías, fueron diseñados los criterios de \textbf{homogeneidad} ($h$) y \textbf{completitud} ($c$) respectivamente.
Estos están dados por las siguientes fórmulas:

\begin{gather}
    h = 1 - \frac{H(X, C|K)}{H(X, C)} \\
    c = 1 - \frac{H(X, K|C)}{H(X, K)}
\end{gather}

\noindent
donde $X$ es el conjunto de datos clasificados por las categorías $C$, y $K$ los clusters obtenidos sobre dicho conjunto.
Para el cálculo de $H(C|K)$ se emplea~\ref{eq:clustering-entropy-conditional} de forma simétrica, es decir, considerando los clusters como categorías y viceversa.

A partir de la homogeneidad y la completitud se puede definir la \textbf{medida V}~\cite{Rosenberg07}, de modo equivalente al que se definió la medida F;
o sea, mediante la fórmula:

\begin{equation}
    \label{eq:V-measure}
    V = \frac{2 \cdot h \cdot c}{h + c}
\end{equation}

De forma semejante a lo que sucede con los respectivos criterios equivalentes a la homogeneidad, completitud y medida V;
estos criterios toman valores ubicados en el intervalo $[0,1]$ donde a medida que un valor es más alto, mejor ha sido evaluado el resultado.

\subsubsection{Información Mutua}

El indicador de información mutua proviene del campo de la Teoría de la Información, y mide el nivel en que una variable aleatoria describe otra.
Puede aplicarse para evaluar la calidad del resultado de un algoritmo de clustering al considerar el propio resultado como una de las variables y las categorías reales como la otra.

Dados $U$ y $V$, conjuntos de clusters y categorías respectivamente, y un conjunto de datos de $n$ elementos;
esta medida se calcula a través de la siguiente expresión~\cite{Aggarawal13,Vinh10}:

\begin{equation}
    \label{eq:mutual-information}
    MI(U,V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}{P(i,j)\log{\left( \frac{P(i,j)}{P(i)P'(j)} \right)}}
\end{equation}

\noindent
donde $P(i) = |U_i|/n$ es la probabilidad de que un objeto elegido aleatoriamente en el conjunto de datos pertenezca a la clase $U_i$, $P'(j) = |V_j|/n$ la probabilidad de que pertenezca a la clase $V_j$, y $P(i,j) = |U_i \cap V_j|/n$ la probabilidad de que forme parte de ambas.

La expresión puede asimismo formularse como sigue:

\begin{equation}
    \label{eq:mutual-information-cardinality}
    MI(U,V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}{\frac{|U_i \cap V_j|}{n}\log{\left( \frac{n|U_i \cap V_j|}{|U_i||V_j|} \right)}}
\end{equation}

Los valores de este medidor se encontrarán en el intervalo $[0,\log{|V|}]$;
donde los valores próximos a 0 indican que ambas variables son independientes, mientras que valores más altos indican una mayor correspondencia entre estas.

Los valores de esta medida crecen a medida que aumenta el número de categorías o clusters, sin importar la relación real existente entre ambas variables~\cite{Vinh10}.
Con el fin de resolver esta situación, se propuso una nueva medida denominada \textbf{información mutua ajustada}, dada por la siguiente fórmula:

\begin{equation}
    \label{eq:adjusted-mutual-information}
    AMI(U,V) = \frac{MI(U,V) - E[MI(U,V)]}{\max{(H(X, U), H(X, V)) - E[MI(U,V)]}}
\end{equation}

\noindent
donde $X$ es el conjunto de datos y $H(X,U)$ y $H(X,V)$ las entropías para los clusters y las categorías respectivamente.
El valor esperado $E[MI(U,V)]$ del medidor de información mutua se calcula según~\cite{Vinh10}:

{\footnotesize
\begin{equation*}
    \label{eq:expected-value-mutual-information}
    E[MI(U,V)] = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}\sum_{c_{ij}=(a_i+b_j-n)^{+}}^{\min{(a_i,b_j)}}
    {
    \frac{c_{ij}}{n}
    \log{\left( \frac{n \cdot c_{ij}}{a_i b_j} \right)}
    \frac{a_i ! b_j ! (n-a_i)!(n-b_j)!}{n!c_{ij}!(a_i-c_{ij})!(b_j-c_{ij})!(n-a_i-b_j+c_{ij})!}
    }
\end{equation*}
}

\noindent
donde $a_i = |U_i|$, y $b_j = |V_j|$.

Este nuevo medidor se encuentra normalizado, por lo que toma valores en el intervalo $[0, 1]$, correspondiéndose, como antes, los más altos a resultados de mejor calidad.

\subsection{Medidas orientadas a la similaridad}\label{subsec:medidasOrientadasALaSimilaridad}

Como se mencionó con anterioridad, las medidas de esta categoría se basan en la premisa de que para todo par de objetos incluidos en un mismo cluster, estos debe pertenecer a la misma categoría y viceversa.
Esta aproximación para la validación del clustering puede ser vista como la comparación de dos matrices~\cite{Tan05}:

\begin{itemize}
    \item La matriz correspondiente a las categorías de los objetos (clustering ideal);
    que tiene, 1 en la posición $(i, j)$ si los objetos $i$ y $j$ forman parte de la misma clase, y 0 de lo contrario.
    \item La matriz correspondiente al conjunto de clusters, donde aparece, de forma semejante, el valor 1 en la posición $(i,j)$ si los objetos $i$ y $j$ pertenecen al mismo cluster, y 0 en caso contrario.
\end{itemize}

La correlación ($\Gamma$), también conocida como \textit{estadística gamma}, entre estas dos matrices se puede tomar como una medida de la calidad del resultado del algoritmo de clustering:

\begin{equation}
    \label{eq:gamma-statistic}
    \Gamma = \frac{N_s - N_d}{N_s + N_d}
\end{equation}

\noindent
donde $N_s$ es la cantidad de posiciones en que coinciden los valores ambas matrices, y $N_d$ la cantidad en que son diferentes.
La correlación oscila entre -1 y 1, correspondiéndose estos extremos con la peor y mejor evaluación respectivamente.

De igual modo, a partir de las $n(n-1)/2$ parejas de objetos que pueden formarse en el conjunto de datos, es posible computar los siguientes valores:

\begin{itemize}
    \item $f_{00}$: número de parejas de objetos que pertenecen a clases y clusters diferentes (posiciones en las que aparece el valor 0 en ambas matrices).
    \item $f_{01}$: número de parejas de objetos que pertenecen a clases diferentes y un mismo cluster (posiciones en las que hay un 0 en la primera matriz y un 1 en la segunda).
    \item $f_{10}$: número de parejas de objetos que pertenecen a la misma clase y clusters diferentes (posiciones en las que hay un 1 en la primera matriz y un 0 en la segunda).
    \item $f_{11}$: número de parejas de objetos que pertenecen a clases y clusters iguales (posiciones en las el valor 1 coincide en las dos matrices).
\end{itemize}

Empleando estos cuatro valores se definen tres de las medidas más usadas para la evaluación de algoritmos de clustering: el \textbf{índice de Rand}, el \textbf{coeficiente de Jaccard} y el \textbf{índice de Fowlkes-Mallows}, dadas por las ecuaciones (\ref{eq:rand-index}), (\ref{eq:jaccard-coefficient}) y (\ref{eq:fowlkes-mallows-index}) respectivamente~\cite{Aggarawal13,Tan05,Wagner07}.

\begin{align}
    \label{eq:rand-index}
    RI & = \frac{f_{00} + f_{11}}{f_{00}+f_{01}+f_{10}+f_{11}} \\
    \label{eq:jaccard-coefficient}
    JC & = \frac{f_{11}}{f_{01}+f_{10}+f_{11}} \\
    \label{eq:fowlkes-mallows-index}
    FMI & = \frac{f_{11}}{\sqrt{(f_{11}+f_{01})(f_{11}+f_{10})}}
\end{align}

Los tres criterios oscilan en el intervalo $[0,1]$, donde valores más altos indican mejores resultados.
El índice de Fowlkes-Mallows además no sufre un crecimiento con el número de clusters, lo que sí ocurre para los otros dos medidores;
aunque, por otra parte, alcanza valores altos cuando la cantidad de clusters es relativamente pequeña~\cite{Wagner07}.
El índice de Rand y el coeficiente de Jaccard pueden ser ajustados siguiendo un procedimiento semejante al aplicado al medidor de información mutua~\cite{Vinh10, Wagner07}.

