\section{Clustering}\label{sec:clustering}

\begin{frame}
    \frametitle{Clustering}

    \begin{itemize}
        \item<1-> Algoritmos de aprendizaje no supervisado que agrupan los elementos de una colección de datos en conjuntos (\textbf{clusters}).
        \item<2-> Simplificar la estructura del conjunto de datos.
        \item<3-> Encontrar grupos de especial significación.
    \end{itemize}

    \begin{itemize}
        \item<3-> ¿Qué constituye un cluster?
        \item<4-> ¿Cómo hallarlos eficientemente?
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Clustering Particional}

    \begin{block}{K-Means}
        \begin{enumerate}
            \item<2-> Seleccionar $K$ puntos como \textit{centroides} iniciales.
            \item<3-> Formar $K$ clusters asignando cada punto al centroide más próximo.
            \item<4-> Recomputar el centroide de cada cluster.
            \item<5-> Repetir los pasos 2 y 3 hasta que se cumpla cierto \textit{criterio de convergencia}.
        \end{enumerate}
    \end{block}

    \begin{itemize}
        \item<6-> Minimizar la \textbf{Suma de Errores Cuadráticos} (SSE).
        \item<7-> Centroides que son la media de sus clusters minimizan la SSE\@.
    \end{itemize}

\end{frame}

\begin{frame}

    \begin{block}{Selección de centroides iniciales}
        \begin{itemize}
            \item<2-> aleatoria
            \item<3-> clustering jerárquico hasta formar $K$ clusters y tomar sus centoides
            \item<4-> \textbf{K-Means++}
        \end{itemize}
    \end{block}

    \pause
    \begin{block}{Estimación del número de clusters}
        \begin{itemize}
            \item<5-> Criterio de Información de Akaike: $K=argmin_{K}[SSE(K)+2M K]$
            \item<6-> Coeficiente de Silueta
            \item<7-> Índice de Calinski-Harabasz
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Clustering Jerárquico}

    \begin{itemize}
        \item<2-> \textbf{Aglomerativos}: Cada punto comienza en un cluster propio.
        Iterativamente se van uniendo hasta alcanzar un único cluster.
        \item<3-> \textbf{Divisivos}: Todos los puntos pertenecen a un mismo cluster inicial.
        De forma iterativa se separa un cluster en dos, hasta obtener clusters de un único punto.
    \end{itemize}

\end{frame}

\begin{frame}

    \begin{block}{Clustering Jerárquico Aglomerativo}
        \begin{enumerate}
            \item<2-> Computar la matriz de proximidad si es necesario.
            \item<3-> Unir los dos clusters \textbf{más próximos}.
            \item<4-> Actualizar la matriz de proximidad en correspondencia con las distancias entre el nuevo cluster y los ya existentes.
            \item<5-> Repetir los pasos 2 y 3 hasta que \textit{exista solamente un cluster}.
        \end{enumerate}
    \end{block}

    \begin{itemize}
        \item<6-> \textit{Single link}.
        \item<7-> \textit{Complete link}.
        \item<8-> \textit{Group averaged} y \textit{Centroid-based}.
        \item<9-> \textit{Método de Ward}.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Densidad}

    \begin{figure}[!h]
        \centering
        \includegraphics[width=\textwidth]{kmeans-dbscan.png}
    \end{figure}

    {\footnotesize
    Resultados de los algoritmos K-Means y DBSCAN ejecutados sobre un conjunto de datos que sigue una distribución \textit{anisotrópica}.
    }

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Densidad}

    \begin{block}{Densidad de un punto}
        Cantidad de puntos localizados alrededor de este en un radio, $Eps$, específico.
        El propio punto es incluido en el conteo.
    \end{block}

    \pause
    \alert{El valor del radio es determinante en la densidad de un punto.}

    \pause
    \begin{figure}[!h]
        \centering
        \includegraphics[width=0.8\textwidth]{dbscan.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Densidad}

    \begin{block}{DBSCAN}
        \begin{enumerate}
            \item<2-> Etiquetar todos los puntos como \textit{núcleo}, \textit{frontera} o \textit{ruido}.
            \item<3-> Eliminar los puntos de ruido.
            \item<4-> Añadir una arista entre todo par de puntos núcleos que se encuentren a una distancia menor o igual que $Eps$.
            \item<5-> Convertir cada componente conexa del grafo resultante en un cluster.
            \item<6-> Asignar cada punto frontera a uno de los clusters de los puntos núcleos asociados a este.
        \end{enumerate}
    \end{block}

\end{frame}

\begin{frame}

    Selección de parámetros para DBSCAN:

    \begin{itemize}
        \item<2-> \textbf{$k$-distancia de un punto}: Distancia al $k$-ésimo punto más cercano a este.
        \item<3-> Los valores de la k-distancias para puntos que pertenezcan a algún cluster no mostrarán un rango de valores muy amplio.
        \item<4-> Para puntos que no pertenezcan a ningún cluster, es decir, de ruido, el valor sí estará situado muy por encima del rango antes mencionado.
        \item<5-> Si se observan en una gráfica las $k$-distancias ordenadas de menor a mayor, el punto de inflexión corresponderá al valor $Eps$.
        \item<6-> $MinPts$ será el $k$ seleccionado para calcular las $k$-distancias.
    \end{itemize}

\end{frame}

\begin{frame}

    \begin{figure}[!h]
        \centering
        \includegraphics[width=0.7\textwidth]{dbscan-k-dist.png}
    \end{figure}

    {\footnotesize
    Representación de los puntos de un conjunto de datos ordenados por su $k$-distancia ($k=4$).
    }

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Densidad}

    \begin{figure}[!h]
        \centering
        \includegraphics[width=\textwidth]{density-issues.png}
    \end{figure}

    {\footnotesize
    Cuatro clusters en un entorno de ruido.
    Los tonos de gris más oscuros indican mayores densidades.
    }

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Densidad}

    \begin{block}{HDBSCAN}
        \begin{enumerate}
            \item<2-> Transformar el espacio en correspondencia con la densidad/dispersión de los puntos.
            \item<3-> Computar el árbol abarcador de costo mínimo correspondiente al grafo completo ponderado por las distancias halladas.
            \item<4-> Construir una jerarquía de componentes conexas (clusters).
            \item<5-> Condensar la jerarquía a partir de un tamaño mínimo para los clusters.
            \item<6-> Extraer los clusters estables del árbol condensado.
        \end{enumerate}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Probabilidades}

\end{frame}

\begin{frame}
    \frametitle{Clustering Basado en Grafos}

\end{frame}